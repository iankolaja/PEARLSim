{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96fbbfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pearlsim.ml_utilities import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de51fd90",
   "metadata": {},
   "source": [
    "# Extract data from Serpent detector files\n",
    "We can use the read_det_file function from ml_utilities to parse all of the detector files we have in the training data directory to create one pair of unified features/target dataframes.\n",
    "\n",
    "Sometimes the file(s) provided will have really high uncertainty. This will absolutely limit your model's accuracy, so don't be disheartened until I get you more accurate data, which can take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f879a419",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training_data/gFHR_core_1.serpent_det0.m'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the auxiliary feature file, which are additionally features I included from\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# the model that tell you more about the respective pebble for each surface\u001b[39;00m\n\u001b[1;32m     11\u001b[0m aux_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_auxiliary_features\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m features, targets, pebble_ids, avg_uncertainty \u001b[38;5;241m=\u001b[39m read_det_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mdet_name)\n\u001b[1;32m     13\u001b[0m aux_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39maux_name, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Read the pebble power file and grab the corresponding power values\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PBR/PEARLSim/pearlsim/ml_utilities.py:21\u001b[0m, in \u001b[0;36mread_det_file\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     19\u001b[0m z_array \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m pebble_flux_matrix \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     23\u001b[0m         line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDET\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_data/gFHR_core_1.serpent_det0.m'"
     ]
    }
   ],
   "source": [
    "training_steps = [1, 22]\n",
    "all_features = pd.DataFrame([])\n",
    "all_targets = pd.DataFrame([])\n",
    "for i in training_steps:\n",
    "    # Load the Serpent Detector file, a matlab format file that has pebble surface flux\n",
    "    # and overall core flux. \n",
    "    det_name = f\"gFHR_core_{i}.serpent_det0.m\"\n",
    "    \n",
    "    # Load the auxiliary feature file, which are additionally features I included from\n",
    "    # the model that tell you more about the respective pebble for each surface\n",
    "    aux_name = f\"current_auxiliary_features{i}.csv\"\n",
    "    features, targets, pebble_ids, avg_uncertainty = read_det_file(\"training_data/\"+det_name)\n",
    "    aux_features = pd.read_csv(\"training_data/\"+aux_name, index_col=0)\n",
    "    \n",
    "    # Read the pebble power file and grab the corresponding power values\n",
    "    pow_name = f\"pebble_positions_{i}.csv_pow0.m\"\n",
    "    pow_data = pd.read_csv(\"training_data/\"+pow_name, delimiter='\\s+', \n",
    "                           names=['x','y','z','rad','universe','power','unc'])\n",
    "    targets['power'] = pow_data['power'].iloc[pebble_ids].reset_index().drop(columns='index')\n",
    "    features = aux_features.join(features)\n",
    "    \n",
    "    # Convert coordinates to 2D cylindrical, since the flux map is symmetrical\n",
    "    features['radius'] = round(np.sqrt(features['x']**2+features['y']**2),1)\n",
    "    features['height'] = round(features['z'],1)\n",
    "    features = features.drop(columns=['x','y','z'])\n",
    "    \n",
    "    print(f\"File {det_name} has an average {round(avg_uncertainty*100,2)}% uncertainty.\")\n",
    "    all_features = pd.concat([all_features, features])\n",
    "    all_targets = pd.concat([all_targets, targets])\n",
    "\n",
    "# For clarity, re-order the radius and height columns to be first and save\n",
    "headers = all_features.columns.to_list()\n",
    "headers.remove(\"radius\")\n",
    "headers.remove(\"height\")\n",
    "all_features = all_features[[\"radius\"]+[\"height\"]+headers]\n",
    "\n",
    "all_features.to_csv(\"training_data/current_data.csv\")\n",
    "all_targets.to_csv(\"training_data/current_target.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccef4d2",
   "metadata": {},
   "source": [
    "Our features include the radius and height of the pebble-based detector. It also includes every bin of the core flux map. The gFHR model is divided into 12 energy groups and has 4 radial divisions, each divided into 10 separate axial zones. Normally you would need to volume-weight these flux values, and deal with the somewhat complicated indexing scheme to sort out which is which. We don't need to bother here, since they're all going to be standardized anyways and the model will make its own inferences about the spatial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.iloc[-10:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0972bb",
   "metadata": {},
   "source": [
    "Meanwhile, the target variables represent the neutron current going into the pebbles above, divided into the same 12 energy bins. There's a tradeoff between how fine our energy grid is, and how long it takes for the Serpent model to run. Every time you add a new bin, you need to simulate more particles to get sufficient statistics. So, our goal is to maximize accuracy using as few bins as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5da15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_plot = 2\n",
    "energies = all_targets.drop(columns=\"power\").columns.to_list()\n",
    "sample_current = all_targets.drop(columns=\"power\").iloc[i_to_plot].to_list()\n",
    "plt.plot(energies, sample_current)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Energy MeV\")\n",
    "plt.ylabel(\"Pebble Surface Current (n/cm^2-s)\")\n",
    "plt.show()\n",
    "print(f\"Pebble power: {all_targets.iloc[i_to_plot]['power']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8209b9c4",
   "metadata": {},
   "source": [
    "# Data Standardization\n",
    "Simple standardization is performed here along each column. I tried log-standardization, but it didn't seem to help with the current values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f8f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "np.random.seed(42)\n",
    "\n",
    "def standardize(raw_data, mean=None, std=None, axis=0):\n",
    "    if mean is None:\n",
    "        mean = np.mean(raw_data, axis = axis)\n",
    "    if std is None:\n",
    "        std = np.std(raw_data, axis = axis)\n",
    "        std[ std==0 ] = 0.1\n",
    "    result = (raw_data - mean) / std\n",
    "    return result, mean, std\n",
    "\n",
    "def unstandardize(standardized_data, mean, std):\n",
    "    raw_data = (standardized_data*std)+mean\n",
    "    return raw_data\n",
    "\n",
    "#log_features = all_features#.apply(lambda x: np.log10(x + 1))\n",
    "#log_targets = all_targets\n",
    "#log_targets.iloc[:,2:] = all_targets.iloc[:,2:]#.apply(lambda x: np.log10(x + 1))\n",
    "\n",
    "num_data = len(all_features)\n",
    "training_size = int(num_data*train_split)\n",
    "testing_size = num_data - training_size\n",
    "data_indices = np.arange(num_data)\n",
    "training_indices = np.random.choice(num_data, training_size, replace=False)\n",
    "testing_indices = data_indices[np.in1d(data_indices, training_indices, invert=True)]\n",
    "\n",
    "training_data, data_mean, data_std = standardize(all_features.iloc[training_indices])\n",
    "training_target, target_mean, target_std = standardize(all_targets.iloc[training_indices])\n",
    "testing_data, _, _  = standardize(all_features.iloc[testing_indices], mean=data_mean, std=data_std)\n",
    "testing_target, _, _  = standardize(all_targets.iloc[testing_indices], mean=target_mean, std=target_std)\n",
    "\n",
    "print(np.shape(training_data))\n",
    "print(np.shape(training_target))\n",
    "print(np.shape(testing_data))\n",
    "print(np.shape(testing_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3758b6",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "I threw together a quick RFR model and got some results. You're free to change to any other type of model, as long as its something I can save and load into other modules. Things to try:\n",
    "- Properly using cross validation\n",
    "- Tuning the hyper parameters\n",
    "- Trying a different model, probably a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'max_depth': 10, \n",
    "               'n_estimators': 1000, \n",
    "               'n_jobs': 14,} # Set to your number of cores\n",
    "rfr_model = RandomForestRegressor(random_state=0)\n",
    "rfr_model.set_params(**best_params)\n",
    "rfr_model.fit(training_data, training_target)\n",
    "rfr_model_test_score = rfr_model.score(testing_data, testing_target)\n",
    "print(f\"RFR score: {rfr_model_test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff0b14",
   "metadata": {},
   "source": [
    "Next we save the models and standardization parameters so the model can be used in the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pickle.dumps(rfr_model)\n",
    "with open(\"ml_models/current_rfr.pkl\", 'wb') as f:\n",
    "    f.write(model_data)\n",
    "data_mean.to_csv(\"ml_models/current_rfr_data_mean.csv\", header=True)\n",
    "data_std.to_csv(\"ml_models/current_rfr_data_std.csv\", header=True)\n",
    "target_mean.to_csv(\"ml_models/current_rfr_target_mean.csv\", header=True)\n",
    "target_std.to_csv(\"ml_models/current_rfr_target_std.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd58878d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
